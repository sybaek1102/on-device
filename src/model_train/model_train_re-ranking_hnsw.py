#!/usr/bin/env python3
"""
model_train_re-ranking_hnsw.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
create_reranking_model_train_data_hnsw.py Î°ú ÏÉùÏÑ±Îêú feature/labelÎ°ú
10-Fold OOF Ïù¥ÏßÑ Î∂ÑÎ•ò MLP Î™®Îç∏ÏùÑ ÌïôÏäµÌï©ÎãàÎã§. (HNSW Î≤ÑÏ†Ñ)

[ÏûÖÎ†•]
  re-ranking_features_hnsw.npz : shape (10000, 33)
                             - [:, :32] : features (PQ dist 16 + Residual dist 16)
                             - [:, 32]  : label (0/1)

[Ï∂úÎ†•]
  model_k1.pt ~ model_k10.pt : Í∞Å Fold val_loss best Î™®Îç∏
  train_log.csv               : epochÎ≥Ñ ÌïôÏäµ Î°úÍ∑∏
  oof_result.npz              : OOF ÌôïÎ•† / ÏòàÏ∏° Î†àÏù¥Î∏î

[Ï†ÄÏû• ÏúÑÏπò]
  data/model/re-ranking_hnsw/
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler
import os
import sys

# =============================================================================
# üîπ Configuration
# =============================================================================
FEATURE_PATH = "/home/syback/vectorDB/on-device/data/model/re-ranking_hnsw/re-ranking_features_hnsw.npz"

MODEL_SAVE_DIR = "/home/syback/vectorDB/on-device/data/model/re-ranking_hnsw"
LOG_PATH       = os.path.join(MODEL_SAVE_DIR, "train_log.csv")
OOF_PATH       = os.path.join(MODEL_SAVE_DIR, "oof_result.npz")

# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
BATCH_SIZE    = 128
LEARNING_RATE = 0.001
MAX_EPOCHS    = 100
THRESHOLD     = 0.5
NUM_FOLDS     = 10

# =============================================================================
# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú & Ï†ÑÏ≤òÎ¶¨
# =============================================================================
print("\n" + "="*70)
print("üìÇ Re-ranking MLP ÌïôÏäµ (10-Fold OOF)")
print("="*70)
print("\n1Ô∏è‚É£  Îç∞Ïù¥ÌÑ∞ Î°úÎìú & Ï†ÑÏ≤òÎ¶¨")

if not os.path.exists(FEATURE_PATH):
    print(f"‚ùå ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {FEATURE_PATH}")
    sys.exit(1)

with np.load(FEATURE_PATH) as f:
    dataset = f["data"]                                # (10000, 33)

X_numpy = dataset[:, :-1].astype(np.float32)          # (10000, 32)
y_numpy = dataset[:,  -1].astype(np.float32).reshape(-1, 1)  # (10000, 1)

print(f"  ‚úì Feature Shape : {X_numpy.shape}")
print(f"  ‚úì Label  Shape  : {y_numpy.shape}")
print(f"  ‚úì Label Dist    - 0: {int(np.sum(y_numpy == 0)):,}  /  1: {int(np.sum(y_numpy == 1)):,}")

# =============================================================================
# 2. OOF Í≤∞Í≥º Ï†ÄÏû• Î∞∞Ïó¥ & Fold Split
# =============================================================================
num_samples = len(X_numpy)
all_indices = np.arange(num_samples)
oof_probs   = np.zeros((num_samples, 1), dtype=np.float32)

fold_chunks = np.array_split(all_indices, NUM_FOLDS)

print(f"\n  ‚úì Total Samples:      {num_samples:,}")
print(f"  ‚úì Samples per Fold: ~{len(fold_chunks[0]):,}")

# =============================================================================
# 3. Model Ï†ïÏùò (21Î≤à Ï∞∏Ï°∞ ÌååÏùºÍ≥º ÎèôÏùºÌïú Íµ¨Ï°∞)
# =============================================================================
class SimpleMLP(nn.Module):
    """
    Input: 32 dims (PQ Dist 16 + OOF-based Residual Dist 16)
    Output: 1 dim (Sigmoid ‚Üí re-ranking ÌïÑÏöî ÌôïÎ•†)
    """
    def __init__(self):
        super(SimpleMLP, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(32, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.network(x)

# Íµ¨Ï°∞ ÌôïÏù∏Ïö© Ï∂úÎ†•
_tmp = SimpleMLP()
print(f"\n  ‚úì Model Architecture:\n{_tmp}")
print(f"  ‚úì Total Parameters: {sum(p.numel() for p in _tmp.parameters()):,}")
del _tmp

# =============================================================================
# 4. 10-Fold OOF ÌïôÏäµ
# =============================================================================
print(f"\n4Ô∏è‚É£  ÌïôÏäµ ÏãúÏûë: {NUM_FOLDS}-Fold OOF, Max Epochs: {MAX_EPOCHS}")

os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
history = []

for fold in range(NUM_FOLDS):
    print(f"\n{'='*70}")
    print(f"üìÇ Fold {fold + 1}/{NUM_FOLDS}")
    print(f"{'='*70}")

    # ‚îÄ‚îÄ Fold Split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    test_idx = fold_chunks[fold]
    val_idx  = fold_chunks[(fold + 1) % NUM_FOLDS]
    train_chunks = [fold_chunks[i]
                    for i in range(NUM_FOLDS)
                    if i != fold and i != (fold + 1) % NUM_FOLDS]
    train_idx = np.concatenate(train_chunks)

    print(f"  Train: {len(train_idx):,}  Val: {len(val_idx):,}  Test: {len(test_idx):,}")

    # ‚îÄ‚îÄ Raw Ïä¨ÎùºÏù¥Ïã± ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    X_train_raw = X_numpy[train_idx]
    X_val_raw   = X_numpy[val_idx]
    X_test_raw  = X_numpy[test_idx]

    # ‚îÄ‚îÄ Feature Í∑∏Î£πÎ≥Ñ ÎèÖÎ¶Ω Scaling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Group 1: PQ Distance (Ïïû 16Í∞ú)
    # Group 2: OOF-based Residual Dist (Îí§ 16Í∞ú)
    scaler_f1 = StandardScaler()
    scaler_f2 = StandardScaler()

    X_train_f1 = scaler_f1.fit_transform(X_train_raw[:, :16])
    X_train_f2 = scaler_f2.fit_transform(X_train_raw[:, 16:])

    X_val_f1 = scaler_f1.transform(X_val_raw[:, :16])
    X_val_f2 = scaler_f2.transform(X_val_raw[:, 16:])

    X_test_f1 = scaler_f1.transform(X_test_raw[:, :16])
    X_test_f2 = scaler_f2.transform(X_test_raw[:, 16:])

    X_train_scaled = np.hstack([X_train_f1, X_train_f2])
    X_val_scaled   = np.hstack([X_val_f1,   X_val_f2])
    X_test_scaled  = np.hstack([X_test_f1,  X_test_f2])

    # ‚îÄ‚îÄ Tensor Î≥ÄÌôò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    X_train_t = torch.tensor(X_train_scaled)
    y_train_t = torch.tensor(y_numpy[train_idx])
    X_val_t   = torch.tensor(X_val_scaled)
    y_val_t   = torch.tensor(y_numpy[val_idx])
    X_test_t  = torch.tensor(X_test_scaled)

    # ‚îÄ‚îÄ Î™®Îç∏ Ï¥àÍ∏∞Ìôî (FoldÎßàÎã§ ÏÉàÎ°ú ÏÉùÏÑ±) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    model     = SimpleMLP()
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    best_val_loss    = float("inf")
    best_epoch       = 0
    best_model_state = None

    # ‚îÄ‚îÄ Epoch Loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    for epoch in range(1, MAX_EPOCHS + 1):
        # Train
        model.train()
        permutation    = torch.randperm(X_train_t.size(0))
        epoch_loss     = 0.0
        tr_probs_list  = []
        tr_labels_list = []

        for i in range(0, X_train_t.size(0), BATCH_SIZE):
            idx      = permutation[i: i + BATCH_SIZE]
            batch_x  = X_train_t[idx]
            batch_y  = y_train_t[idx]

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss    = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            tr_probs_list.append(outputs.detach().cpu().numpy())
            tr_labels_list.append(batch_y.detach().cpu().numpy())

        avg_train_loss     = epoch_loss / max(1, len(X_train_t) // BATCH_SIZE)
        train_probs_cat    = np.concatenate(tr_probs_list)
        train_labels_cat   = np.concatenate(tr_labels_list)
        train_auc          = roc_auc_score(train_labels_cat, train_probs_cat)
        train_preds        = (train_probs_cat >= THRESHOLD).astype(int)
        train_acc          = accuracy_score(train_labels_cat, train_preds)
        tr_prec, tr_rec, _, _ = precision_recall_fscore_support(
            train_labels_cat, train_preds, average=None, zero_division=0)

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_t)
            val_loss    = criterion(val_outputs, y_val_t).item()
            val_probs   = val_outputs.cpu().numpy()
            val_labels  = y_val_t.cpu().numpy()

        val_auc  = roc_auc_score(val_labels, val_probs)
        val_preds= (val_probs >= THRESHOLD).astype(int)
        val_acc  = accuracy_score(val_labels, val_preds)
        val_prec, val_rec, _, _ = precision_recall_fscore_support(
            val_labels, val_preds, average=None, zero_division=0)

        # Log
        log_entry = {
            "fold": fold + 1, "epoch": epoch,
            "train_loss": avg_train_loss, "train_acc": train_acc, "train_auc": train_auc,
            "train_prec0": tr_prec[0],  "train_rec0": tr_rec[0],
            "train_prec1": tr_prec[1],  "train_rec1": tr_rec[1],
            "val_loss": val_loss, "val_acc": val_acc, "val_auc": val_auc,
            "val_prec0": val_prec[0],   "val_rec0": val_rec[0],
            "val_prec1": val_prec[1],   "val_rec1": val_rec[1],
        }
        history.append(log_entry)

        # Best model Ï≤¥ÌÅ¨ & Î©îÎ™®Î¶¨ Ï†ÄÏû•
        if val_loss < best_val_loss:
            best_val_loss    = val_loss
            best_epoch       = epoch
            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}

        # ÏΩòÏÜî Ï∂úÎ†• (10 epochÎßàÎã§)
        if epoch % 10 == 0 or epoch == 1:
            print(f"  Epoch [{epoch:3d}/{MAX_EPOCHS}] "
                  f"Loss: {avg_train_loss:.4f}/{val_loss:.4f} | "
                  f"AUC: {train_auc:.4f}/{val_auc:.4f} | "
                  f"Acc: {train_acc:.4f}/{val_acc:.4f}")

    # ‚îÄ‚îÄ Best model ÎîîÏä§ÌÅ¨ Ï†ÄÏû• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print(f"\n  ‚úì Best Epoch: {best_epoch}  Best Val Loss: {best_val_loss:.4f}")
    model.load_state_dict(best_model_state)

    model_path = os.path.join(MODEL_SAVE_DIR, f"model_k{fold + 1}.pt")
    torch.save(model.state_dict(), model_path)
    print(f"  ‚úì Model saved: {model_path}")

    # ‚îÄ‚îÄ Scaler ÌååÎùºÎØ∏ÌÑ∞ Ï†ÄÏû• (inference Ïû¨ÌòÑÏö©) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    scaler_path = os.path.join(MODEL_SAVE_DIR, f"scaler_k{fold + 1}.npz")
    np.savez(scaler_path,
             f1_mean=scaler_f1.mean_,   f1_std=np.sqrt(scaler_f1.var_),
             f2_mean=scaler_f2.mean_,   f2_std=np.sqrt(scaler_f2.var_))
    print(f"  ‚úì Scaler saved: {scaler_path}")

    # ‚îÄ‚îÄ Test (OOF) Prediction (best model Í∏∞Ï§Ä) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_t)
        oof_probs[test_idx] = test_outputs.cpu().numpy()

    print(f"  ‚úì Fold {fold + 1} ÏôÑÎ£å!")

# =============================================================================
# 5. Í≤∞Í≥º Ï†ÄÏû•
# =============================================================================
print("\n" + "="*70)
print("5Ô∏è‚É£  Í≤∞Í≥º Ï†ÄÏû•")
print("="*70)

pd.DataFrame(history).to_csv(LOG_PATH, index=False)
print(f"‚úì ÌïôÏäµ Î°úÍ∑∏ Ï†ÄÏû•: {LOG_PATH}")

oof_pred_labels = (oof_probs >= THRESHOLD).astype(np.float32)
np.savez_compressed(OOF_PATH, pred_prob=oof_probs, pred_label=oof_pred_labels)
print(f"‚úì OOF Í≤∞Í≥º Ï†ÄÏû•:  {OOF_PATH}")
print(f"  - pred_prob  shape: {oof_probs.shape}")
print(f"  - pred_label shape: {oof_pred_labels.shape}")

# =============================================================================
# 6. Ï†ÑÏ≤¥ OOF ÏÑ±Îä• ÌèâÍ∞Ä
# =============================================================================
print("\n" + "="*70)
print("6Ô∏è‚É£  Ï†ÑÏ≤¥ OOF ÏÑ±Îä• (10-Fold)")
print("="*70)

oof_auc  = roc_auc_score(y_numpy, oof_probs)
oof_acc  = accuracy_score(y_numpy, oof_pred_labels)
oof_prec, oof_rec, _, _ = precision_recall_fscore_support(
    y_numpy, oof_pred_labels, average=None, zero_division=0)

print(f"  Overall OOF Accuracy  : {oof_acc:.4f}")
print(f"  Overall OOF AUC       : {oof_auc:.4f}")
print(f"  Class 0 Precision     : {oof_prec[0]:.4f}")
print(f"  Class 0 Recall        : {oof_rec[0]:.4f}")
print(f"  Class 1 Precision     : {oof_prec[1]:.4f}")
print(f"  Class 1 Recall        : {oof_rec[1]:.4f}")

print("\n" + "="*70)
print("[Feature Íµ¨ÏÑ±]")
print("  Index  0~15 : PQ Distance     (16 dims)  ‚Äî ||Q-P||¬≤ per candidate")
print("  Index 16~31 : Residual Dist   (16 dims)  ‚Äî ||X-P||¬≤ - 2¬∑pred(dot(Q-P,X-P))")
print("  Total: 32 dims")
print("="*70)
print(f"\n‚úÖ ÌïôÏäµ ÏôÑÎ£å!")
print(f"   Ï†ÄÏû•Îêú Î™®Îç∏: model_k1.pt ~ model_k{NUM_FOLDS}.pt")
print(f"   Ï†ÄÏû• Í≤ΩÎ°ú:   {MODEL_SAVE_DIR}")
